Structured Programming - Djikstra, Hoare

context: 1972, low-level programming, lots of goto, no high-level language, 1968 article "goto statement considered harmful"

Nomenclature:

Normal text represents my general notes and understanding of a chapter.
-> text prefixed with a simple arrow is a note on a specific paragraph
=> text prefixed with a double arrow is my interpretation of the chapter's conclusion
(text in parentheses is my own reaction to something that is being read)
"text in double quotes is, you guessed it, a quote"
("sometimes, there are quotes in parentheses to remind myself what some things mean")
...Text following ellipses is the start of the first non-read paragraph

Notes by chapter/section:
I) Notes on structured programming
1) To my reader
This was first written as personal notes and passed privately.

2) On our inability to do much
We delude ourselves thinking that we can apply the same solutions to big and small programs and that scale is an irrelevant factor.
=> Size matters

3) On the Reliability of Mechanism
-> Checking a mechanism's correctness from only inputs and outputs is a fool's errand for big machin.
-> You need to take the structure of the mechanism into account.
-> A program is a mechanism and the same rules of checking for correctness apply: you need to check the program's structure.
-> For a highly composite program to be correct, its component must be correct (?)
-> It's not enough to program something that works, you need to program something that demonstrably works right (tests? format proofs?)
-> Concerns of the programmer regarding code: correctness, adaptability, manageability
-> The more powerful machines become, the less we try to make our algorithms better; we rely on the machines power rather than efficient programs. Or: Parkinson's law ("work expands so as to fill the time available for its completion") can be extended to programs.
-> Machines will continue to increase in power and the things we should be able to do with them should also grow, not remain stagnant.
-> "the art of programming is the art of organising complexity, of mastering multitude and avoiding its bastard chaos as effectively as possible"
-> optimisation and manageability can be tradeoffs. Programs should be manageable.
-> Program testing only shows the presence of bugs, never their absence.
-> If we care about program correctness, testing by cases is anecdotal evidence. 
(what about property testing ?)

=> This seems like it's more about imperative programming? The need to know about a program's structure at least.

4) On our Mental aids
-> How do we convince ourself and others that we should care about program structure and how do we make good program structure?
-> mental aids: enumeration, mathematical induction, abstraction.

4.1) On enumeration
-> "enumerative reasoning" when a property of compatutation is like a sequential enumerated set of statements
-> a computation gives a enumerated number of outcomes ( x < r is either true or false, each branch of the if has that statement as an invariant)

=> enumeration looks like match case with guards but those don't check the described invariants in the following branch

4.2) on mathematical induction
-> for loops/recursions
-> a description of how while loops and recursion finishes on condition, 
  that a code will loop k times >= 0  ==>  that it won't terminate on loop  0 <= n < k
-> these proofs are too long to write and they're annoying!
-> it shouldn't be the duty of the programmer to supply them!
-> obvious is obvious because familiarity. obvious strucs may need name rather than be consciously appealed (to by structure ?) 
-> no set of useful theorems to appeal to (? written in ~1970). Intuitive performance may be limiting yourself to known theorems/constructs, but some problems may need other theorems.
-> the length of the proof is a sign that programming is actually complex and a warning against "clever constructions" than may be difficult to prove correct

4.3) on abstraction
-> abstraction is everywhere in programming which makes it hard to talk about as even variable assignment is one
-> naming an operation (=code, a function) and using it is also an abstraction. 
-> There is an analogy between applying functions and using theorems. You can ignore how they were defined/proved and only use their result.
-> Abstraction is more powerful than enumeration as a mental aid
-> "There is a parallel analogy between the unanalysed terms in which an axiom or theorem is 
expressed and the unanalysed operands upon which a named operation is expected to act."

5) An example of a correctness proof
(imagine a proof of a program that takes two values, does some useless computations, and returns the first value `mod` the second )
(followed by several similar proofs)

6) On the validity of proofs vs the validity of implementations
-> proofs only valid in the face of implementation: previous proofs suppose no upper bounds on integers, so how valid are these proofs?
-> proofs in perfect world can fail in imperfect world. Correct proofs should assume the correct axioms (int32 < 2 147 483 647 for instance)
-> be aware of assumptions when dealing with programs
-> a good program should have as little assumptions as possible
-> a good implementation should satisfy as many requirements as possible
(this allows them to meet in the middle between software and hardware)
-> example of failure when writing algol60 where x = y was true if x and y differed in the least significant digit to allow for leniency regarding rounding accuracy.
-> with that definition of =, you could have a = b and b = c while a != c (how? if it's a single digit this rounding should work?)

7) On understanding programs
-> we teach programming languages as if they were a goal instead of a means
-> programming is not the goal, the correct computations are
-> saying a program is correct is saying that the computations described by the program are correct
-> a program is static where the computation is dynamic (a program is a music sheet and the computation is music)
-> Djikstra wants to shorten the gap between program and computation so that assertions about program are true about the computations 
-> structure of program needs to reflect then structure of computation (? what about abstraction? what about code optimizations from the compiler?)
-> Following section limited to programs written for sequential machines
-> def:
  - the purpose of a computation is to produce a desired effect
  - since it happens through time the effect can be measured by looking at the difference between the state at t0 and at t1 when it ended
  - if there are no intermediate states it's a primitive action
-> when we have the intermediate effects it's a sequential computation.
-> we have to convince ourselves that the composite action is the desired net effect of the primitive actions
-> WE HAVE TO THANK HOARE FOR CASE OF'S! THANK YOU HOARE!
-> an example through parsing : decomposition into a fixed number of subaction that can be enumerated 
-> "if ... then" makes it seem like it is either a composite action or no action
-> "if... then... else" and "case ... of" describes things as single actions (not primitive)
-> this is represented by flowchats with single entries and single exits.
-> same is true for "while ... do" and "do ... while"
-> => 3 types of decomposition : concatenation, selection, repetition
-> concatenation & selection can be understood by enumeration, repetition by induction
-> proposal to limit ourselves when programming sequentially to these control structures
-> goals of the proposal:
  - make the mental effort to understand a program only proportional to the program length
  - we can understand (sufficiently small) concatenation via enumerative reasoning, which then allows us to make assertions about each step.
  - if a step is controlled by a selection the specific path is irrelevant wrt the primary level (the step still has a single exit)
  - if a step is controlled by a repetition, the number of repetition is irrelevant wrt the primary level (same thing, on the condition that repetition happens the correct finite number of times)
  - we can understand both the selection and repetition clauses thanks to enumerative and inductive reasoning
-> remark: the validity of an assertion about a program's result can't be guaranteed during its execution
```
dd := dd / 2
if dd <= r do r := r - dd
```
implies 
`0 <= r < dd`
but before the first step the following assertion can be true:
`dd >= r`
-> the validity of a relation is dependent on the progress of comutation, and this is typical for sequential processes
-> having actions that are sequential (instead of simultaneous) may create some invalid states (reseting the number of printed lines after OR BEFORE changing pages leaves you either with printed lines and a state at 0, or a state with an arbitrary non-zero number of printed lines but no printed lines for the current pages)
-> how do we characterise the progress of computation?
-> => we try to find "a coordinate system where discrete points of computation progress can be identified, and this system has to be independent of the variables operated upon under program control"
-> if the coordinates aren't independent, then we're assuming our program is already correct rather than checking if it is
-> it would also make it possible to get into an infinite loop where the state doesn't change from one computation to the other, where if the coordinates are dependent (in the state) they haven't changed and the state cannot distinguish between the two points of progress.


...We can state our problem in another way.



