Structured Programming - Djikstra, Hoare

context: 1972, low-level programming, lots of goto, no high-level language, 1968 article "goto statement considered harmful"

Nomenclature:

Normal text represents my general notes and understanding of a chapter.
I1i) text prefixed by a number in arabic or roman numerals followed by a closing parenthesis indicates a new chapter, with the name of the chapter following the parenthesis.
-> text prefixed with a simple arrow is a note on a specific idea or paragraph
=> text prefixed with a double arrow is my interpretation of the chapter's conclusion
(text in parentheses is my own reaction to something that is being read, sometimes a reminder, sometimes a comment)
"text in double quotes is, you guessed it, a quote"
("sometimes, there are quotes in parentheses to remind myself what some things mean")
...Text following ellipses is the start of the first non-read paragraph
###
TEXT BETWEEN TWO LINES CONSISTING ONLY OF # SYMBOLS ARE IMPORTANT INFORMATION UP TO THE NEXT CHAPTER
###

Notes by chapter/section:
I) Notes on structured programming
1) To my reader
This was first written as personal notes and passed privately.

2) On our inability to do much
We delude ourselves thinking that we can apply the same solutions to big and small programs and that scale is an irrelevant factor.
=> Size matters

3) On the Reliability of Mechanism
-> Checking a mechanism's correctness from only inputs and outputs is a fool's errand for big machin.
-> You need to take the structure of the mechanism into account.
-> A program is a mechanism and the same rules of checking for correctness apply: you need to check the program's structure.
-> For a highly composite program to be correct, its component must be correct (?)
-> It's not enough to program something that works, you need to program something that demonstrably works right (tests? format proofs?)
-> Concerns of the programmer regarding code: correctness, adaptability, manageability
-> The more powerful machines become, the less we try to make our algorithms better; we rely on the machines power rather than efficient programs. Or: Parkinson's law ("work expands so as to fill the time available for its completion") can be extended to programs.
-> Machines will continue to increase in power and the things we should be able to do with them should also grow, not remain stagnant.
-> "the art of programming is the art of organising complexity, of mastering multitude and avoiding its bastard chaos as effectively as possible"
-> optimisation and manageability can be tradeoffs. Programs should be manageable.
-> Program testing only shows the presence of bugs, never their absence.
-> If we care about program correctness, testing by cases is anecdotal evidence. 
(what about property testing ?)

=> This seems like it's more about imperative programming? The need to know about a program's structure at least.

4) On our Mental aids
-> How do we convince ourself and others that we should care about program structure and how do we make good program structure?
-> mental aids: enumeration, mathematical induction, abstraction.

4.1) On enumeration
-> "enumerative reasoning" when a property of compatutation is like a sequential enumerated set of statements
-> a computation gives a enumerated number of outcomes ( x < r is either true or false, each branch of the if has that statement as an invariant)

=> enumeration looks like match case with guards but those don't check the described invariants in the following branch

4.2) on mathematical induction
-> for loops/recursions
-> a description of how while loops and recursion finishes on condition, 
  that a code will loop k times >= 0  ==>  that it won't terminate on loop  0 <= n < k
-> these proofs are too long to write and they're annoying!
-> it shouldn't be the duty of the programmer to supply them!
-> obvious is obvious because familiarity. obvious strucs may need name rather than be consciously appealed (to by structure ?) 
-> no set of useful theorems to appeal to (? written in ~1970). Intuitive performance may be limiting yourself to known theorems/constructs, but some problems may need other theorems.
-> the length of the proof is a sign that programming is actually complex and a warning against "clever constructions" than may be difficult to prove correct

4.3) on abstraction
-> abstraction is everywhere in programming which makes it hard to talk about as even variable assignment is one
-> naming an operation (=code, a function) and using it is also an abstraction. 
-> There is an analogy between applying functions and using theorems. You can ignore how they were defined/proved and only use their result.
-> Abstraction is more powerful than enumeration as a mental aid
-> "There is a parallel analogy between the unanalysed terms in which an axiom or theorem is 
expressed and the unanalysed operands upon which a named operation is expected to act."

5) An example of a correctness proof
(imagine a proof of a program that takes two values, does some useless computations, and returns the first value `mod` the second )
(followed by several similar proofs)

6) On the validity of proofs vs the validity of implementations
-> proofs only valid in the face of implementation: previous proofs suppose no upper bounds on integers, so how valid are these proofs?
-> proofs in perfect world can fail in imperfect world. Correct proofs should assume the correct axioms (int32 < 2 147 483 647 for instance)
-> be aware of assumptions when dealing with programs
-> a good program should have as little assumptions as possible
-> a good implementation should satisfy as many requirements as possible
(this allows them to meet in the middle between software and hardware)
-> example of failure when writing algol60 where x = y was true if x and y differed in the least significant digit to allow for leniency regarding rounding accuracy.
-> with that definition of =, you could have a = b and b = c while a != c (how? if it's a single digit this rounding should work?)

7) On understanding programs
-> we teach programming languages as if they were a goal instead of a means
-> programming is not the goal, the correct computations are
-> saying a program is correct is saying that the computations described by the program are correct
-> a program is static where the computation is dynamic (a program is a music sheet and the computation is music)
-> Djikstra wants to shorten the gap between program and computation so that assertions about program are true about the computations 
-> structure of program needs to reflect then structure of computation (? what about abstraction? what about code optimizations from the compiler?)
-> Following section limited to programs written for sequential machines
-> def:
  - the purpose of a computation is to produce a desired effect
  - since it happens through time the effect can be measured by looking at the difference between the state at t0 and at t1 when it ended
  - if there are no intermediate states it's a primitive action
-> when we have the intermediate effects it's a sequential computation.
-> we have to convince ourselves that the composite action is the desired net effect of the primitive actions
-> WE HAVE TO THANK HOARE FOR CASE OF'S! THANK YOU HOARE!
-> an example through parsing : decomposition into a fixed number of subaction that can be enumerated 
-> "if ... then" makes it seem like it is either a composite action or no action
-> "if... then... else" and "case ... of" describes things as single actions (not primitive)
-> this is represented by flowchats with single entries and single exits.
-> same is true for "while ... do" and "do ... while"
-> => 3 types of decomposition : concatenation, selection, repetition
-> concatenation & selection can be understood by enumeration, repetition by induction
-> proposal to limit ourselves when programming sequentially to these control structures
###
THE FOLLOWING IS ONLY TRUE FOR SEQUENTIAL PROGRAMS
###
-> goals of the proposal:
  - make the mental effort to understand a program only proportional to the program length
  - we can understand (sufficiently small) concatenation via enumerative reasoning, which then allows us to make assertions about each step.
  - if a step is controlled by a selection the specific path is irrelevant wrt the primary level (the step still has a single exit)
  - if a step is controlled by a repetition, the number of repetition is irrelevant wrt the primary level (same thing, on the condition that repetition happens the correct finite number of times)
  - we can understand both the selection and repetition clauses thanks to enumerative and inductive reasoning
-> remark: the validity of an assertion about a program's result can't be guaranteed during its execution
```
dd := dd / 2
if dd <= r do r := r - dd
```
implies 
`0 <= r < dd`
but before the first step the following assertion can be true:
`dd >= r`
-> the validity of a relation is dependent on the progress of comutation, and this is typical for sequential processes
-> having actions that are sequential (instead of simultaneous) may create some invalid states (reseting the number of printed lines after OR BEFORE changing pages leaves you either with printed lines and a state at 0, or a state with an arbitrary non-zero number of printed lines but no printed lines for the current pages)
-> how do we characterise the progress of computation?
-> => we try to find "a coordinate system where discrete points of computation progress can be identified, and this system has to be independent of the variables operated upon under program control"
-> if the coordinates aren't independent, then we're assuming our program is already correct rather than checking if it is
-> it would also make it possible to get into an infinite loop where the state doesn't change from one computation to the other, where if the coordinates are dependent (in the state) they haven't changed and the state cannot distinguish between the two points of progress.
-> how can one get the state of a composite computation just before a failure (without a complete memory dump) ?
-> suppose the program text is linear and assume an identifying mechanism for program points/statement executions (using, for instance, semicolons) : the textual index
-> while restricted to concatenation and selection, a single textual index is sufficient
-> with repetition, it isn't anymore, but we could introduce a dynamic index (using a stack for instance)
-> with procedures, a single textual index isn't sufficient; we need to know to which call of the procedure we refer
-> need a stack of textual indices, instead of a single textual index; one per procedure call, reduced by one at procedure return
-> independent, outside of the programmer's control (compiler-space)
-> a similar coordinate system could work with gotos by counting the discrete points of computations but it would be unhelpful without a textual index
-> CONCLUSION: by restricting ourselves to enumeration & induction on sequential programs, we can use and benefit a systematic error detection system, so we should
-> progress through computation is then mapped on progress through text, making understanding programs easier

8) ON COMPARING PROGRAMS
-> given any single problem, there is no unique program to solve it
-> how can you saw two programs do the same thing? (given two programs p1 and p2 that solve a problem, p1 and p2 may invoque different computations)
-> how can we structure these two programs to simplify comparing them?
-> two programs that have the same effect ("solve the same problem") are only equivalent in that specific regard an in no other a priori
-> it is only interesting to compare programs when you can pair computations in the order they are done and when the program text can equally be parsed into instructions corresponding to actions
(<= dependent on having code and computation being close to equivalent)
-> Strong condition
-> ex1:
```
if B2 then
begin while B1 do S1 end
  else
begin while B1 do S2 end

// vs
while B1 do
begin if B2 then S1 else S2 end
```
-> 1s is "if a then loop b else loop c"
-> 2s is "loop (if a then b else c)"
-> "(1) and (2) are "hard to compare" " => the choice is a relevant design decision to be considered

ex2:
```
j:=0; equal:=true;
while j != N do
begin j:= j + 1; equal:=equal and (X[j] == Y[j]) end

// vs

j:=; equal:=true;
while j != N and equal do
begin j:= j + 1; equal(X[j] == Y[j]) end
```
-> (4) terminates as early as inequality is proven, contrary to (3) which checks the whole array anyway
-> the programs are comparable as the last 2 lines are describing a single action, not divided in sub actions.
-> properties: 
for 0 <= j <= N
for j = 0 => EQUALJ = true
for 0 < j <= N => EQUALJ = EQUALJ-1 And (X[j]==Y[j])
-> net effect: attaining equal = EQUALN by maintaining the relation equal = EQUALJ iteratively through j increasing

-> we may be tempted to consider them both as refinments of a third program (7)
```
j: = O; equal:= EQUAL 0 ;
while "perhaps still:equal != EQUALN" do
beginj: = j +I; "equal:= EQUALJ' end
```
-> where "perhaps still: equal != EQUALN" stands for some sort of still open primitive
-> (3) keeps the relation and can abbreviate equal:=EQUALJ as equal:= equal AND (X[j] == Y[j]) 
-> (4) keeps the relation and can abbreviate equal:=EQUALJ as equal:= (X[j] == Y[j])
-> => THIS MEANS (3) AND (4) ARENT A REFINEMENT OF (7) BECAUSE THERE IS NO UNIFIED WAY TO ABBREVIATE equal:=EQUALJ COMMON TO THEM BOTH TO REPLACE equal:=EQUALJ and there is no one-to-one correspondance
(note: considering something like a dependent bool 
```
data DBool where
  DTrue :: Nat n => DBool n
  DFalse :: Nat n => DBool n

dAnd :: Nat n => DBool n -> Bool -> DBool (n + 1)
dAnd DTrue True = DTrue
dAnd DFalse True = DFalse
dAnd DTrue False = DFalse
dAnd DFalse False = DFalse
```
you could write your `equal != EQUALN` by expecting a `DTrue (length X)`
)

-> THERE ARE "INCOMPARABLE" PROGRAMS
-> comparing programs with one-to-one comparison of actions does not mean the actions should have the same net effect
-> we can use this method to compare both alternative programs for the same job and different programs for similar jobs

9)A FIRST EXAMPLE OF STEP-WISE PROGRAM COMPOSITION
-> systematic sequencing => the structure of computations is reflected in our program
-> example of construction of a well-structured program
-> instruct a computer to print a table of the first thousand prime numbers, 2 being the first.
-> example chosen because it is both complex enough to serve as model and simple enough that our attention is not usurped by the problem
-> a well-structured program is not guaranteed to be "the best program"
-> iterative approach by doing the least possible improving step 
-> simplest program is "begin `print first thousand prime numbers` end " (we suppose the function exists)
-> no such primitive
```
begin variable "table p";
  "fill table p with first thousand prime numbers";
  "print table p"
end
```
-> no such primitive as "fill table p with first thousand prime numbers" so need to go deeper
-> pause: realise about the process that:
-> lots of assumptions: 
  - "table p" is an available resource
  - printing and computing primes are independent computations
  - at least a thousand primes exist
-> lots of decisions left:
  - we may not know what prime numbers are at this point of the program
  - we haven't decided on a printing layout
-> strength of the approach that we can make these decisions independent from the problem and that they're left in the refinement of the constituant actions
-> successful use of "divide and rule" (here: first decide what, then how ?)
-> stop pause
-> are the two computations independant? can they be written independently by two programmers?
-> what's table p? what data structure are we using? how and when are we passing the data from action 1 to action 2
-> two ways :
-> 1) postpone this decision and refine the one or both actions, assuming a set of operations on the possible table p
-> 2) decide first what's the type, then refinements of both actions can be done independently
-> both are risky: 2) may make the subcomputations awkward by choosing poorly, and 1) may cause a great algorithm to be terrible to implement for a given data structure
-> the algorithm and the data structure must be well-matched for a good program
-> opinion: the efficient programmer tries to take the easiest decision first, which requires the minimum amount of investigation for the maximum effect
-> assume we chose the type of table p first and examine 2 options
-> an Array Bool where true indicates the index is prime
-> an Array Int with the sequential prime integers
-> assume we chose 2) because it hopefully makes printing easier and keeps previous prime numbers which are useful to determine if a number is prime
-> => table p = integer array p[1:1000] with p[k] the k-th prime number
-> need to initialize table p in our program
-> three, not two instructions
-> first program = description 0; line "print first 1k prime numbers" is action 0a
-> next program = description 1; contains a refinement of 0a expressed in 3 actions 1a 1b 1c
```
table p                     // 1a
fill p with first 1k primes // 1b
print table p               // 1c
```
-> now we go further and describe our convention
-> we call this description 2 =
```
1a = "integer array p[1:1000]"
1b = "make for k from 1 through 1000 p[k] equal to the kth prime number"
1c = "print p[k] for k from 1 through 1000"
```
-> description 2: "for 1a, 1b, 1c, we have chosen the refinements 2a, 2b, 2c"
-> remark: 2c here is a computation that only depends on p's type. It doesn't need to be prime numbers. 
-> follow-up: You shouldn't overlook this generalisation when designing an interface: the conception of the class of possible actions.
-> if 2a is available and 2b and 2c work our problem is resolved
-> suppose this is not the case and we need to conceive subcomputations for 2b and 2c. Thanks to the introduction of level 2, these can be designed independently.
-> refine 2b into 2b1 (2b1 and not 3b to indicate the independance of the refinement):
-> stupid but valid version where the programmer doesn't need the machine 2b1(1):
```
begin p[1]:=2; p[2]:=3;... end
```
-> next best 2b1(2)
```
begin integer k,j; k:=0; j:=1;
  while k < 1000 do begin "increase j until next prime number";
    k:=k+1; p[k]:=j end
end
```
-> would work, need to refine "increase j until next prime number" == 2b1(2)a;
```
begin boolean jprime;
  repeat j:=j+1;
    "give to jprime the meaning: j is a prime number"
  until jprime
end
```
-> remark: repeat-until to indicate that j has to be increased at least once
-> if programmer realises all primes except 2 are odd, he may refine again to improve efficiency
-> 2b1(3)
```
begin integer k,j; p[1]:=2; k:=1; j:=1;
  while k < 1000 do
    begin "increase odd j until next odd prime number"
  end
end
```
-> with 2b1(3)a
```
begin boolean jprime;
  repeat j:=j+2;
    "give for odd j to jprime the meaning: j is a prime number";
  until jprime
end
```
... p35 (this seems more like a deep analysis of the process of building a program, not everything may need to be noted and remembered but the process is very interesting to follow)

actually now p35 but couldn't take notes in the meantime
